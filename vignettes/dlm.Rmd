---
title: "Normal Dynamic Linear Models"
author: "AndrÃ© F. B. Menezes"
date: "Last compiled on `r format(Sys.time(), '%B %d, %Y')`"
header-includes:
    - \usepackage{setspace}
    - \usepackage{float}
    - \usepackage{geometry}
    - \geometry{a4paper,nohead,left=2.0cm,right=2.0cm,bottom=2.5cm,top=2.5cm}
    - \onehalfspacing
fontsize: 12pt
indent: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 13,
  fig.height = 6.5)
library(ggplot2)
library(cowplot)
theme_set(
  theme_cowplot(font_size = 16, font_family = "Palatino") +
    background_grid() +
    theme(legend.position = "top")
)
```


# Introduction

This vignette will show examples of univariate Normal Dynamic Linear Models
(DLMs) in real application using the `RBATS` package.


```{r setup, message=FALSE}
library(RBATS)
```


# Bayesian Normal Dynamic Linear Models

Let the information set be represented as $D_t = \{D_0, \ldots, D_{t-1}\}$ at
any time, with $D_0$ referring to the initial prior information at $t=0$.
Assuming that $(\boldsymbol{\theta}_0 \mid D_0) \sim N[\mathbf{m}_0, \mathbf{C}_0]$,
for some known moments $\mathbf{m}_0$ and $\mathbf{C}_0$, the general normal
DLM is defined by:
\begin{eqnarray}\label{eq:dlm_equations}
Y_t &=& \mathbf{F}^\top_t\,\boldsymbol{\theta}_t + \nu_t, \quad \quad \nu_t \sim N[0, V_t]  \\
\boldsymbol{\theta}_t &=& \mathbf{G}_t\,\boldsymbol{\theta}_{t-1} + \boldsymbol{\omega}_t, \quad \boldsymbol{\omega}_t \sim N[\mathbf{0}, \mathbf{W}_t]
\end{eqnarray}
where $Y_t$ is the observation at time $t$, $\mathbf{F}_t$ is a $p\times 1$ 
regression vector with known constants at time $t$, $\nu_t$ is the observation
error at time $t$, $\boldsymbol{\theta}_t$ is a $p\times 1$ state vector at 
time $t$, $\boldsymbol{\omega}_t$ is the state evolution error, $\mathbf{G}_t$
is a $p\times p$ known matrix describing the state evolution, $V_t$ is the 
observation variance, and $\mathbf{W}_t$ is the evolution variance-covariance
matrix. This model is describe by the quadruple 
$\{\mathbf{F}_t, \mathbf{G}_t, V_t, \mathbf{W}_t\}$.

Two major model types can be distinguished: time series models where 
$\mathbf{F}_t=\mathbf{F}$ and $\mathbf{G}_t=\mathbf{G}$; and dynamic regression 
models where $\mathbf{F}_t=(X_{t,1},\ldots,X_{t,p})'_t$ and 
$\mathbf{G}=\mathbf{I}_p$, where $X_{t,i}$ denotes the $i$ covariate at time
$t$ and $\mathbf{I}_p$ indicates the identity matrix of order $p$.

## Model components

> This section explains the details on how to construct dynamic models and the types of dynamic models available in the `RBATS` package. The model is construct through the `dlm` function.

DLMs are defined as the combination of simpler components based on the 
superposition principle to make it simple to identify process features. The 
superposition of $r \ge 1$ sub-models represented by
$\{\mathbf{F}_i, \mathbf{G}_i, V_i, \mathbf{W}_i\}_t$, for $i=1, \cdots, r$, can be used to specify a 
DLM. By defining $\mathbf{F}_t^{\prime}=(\mathbf{F}_1^{\prime}, \cdots, \mathbf{F}_r^{\prime}), \mathbf{G}_t=\textrm{diag}(\mathbf{G}_1, \cdots, \mathbf{G}_r)_t$ and $\mathbf{W}_t=(\mathbf{W}_1, \cdots, \mathbf{W}_r)_t$, we can obtain the model 
described by the $r$ components.

In fact, the current version of `RBATS` package implements three main model
components: polynomial, seasonal and regressors. In the sequel, we will show
how to define those models throughout the `dlm` function.

### Nonstationary polynomial trend models

These models are special case of time series model, where
$\mathbf{F}_t=\mathbf{F}$ and $\mathbf{G}_t=\mathbf{G}$. The simplest model
is the local level, where $\mathbf{F} = \mathbf{F} = 1$ and the scalar 
vector $\theta_t$ represents the expected level of the series at time $t$, that is
$$
Y_t = \theta_t + \nu_t \quad \mathrm{and} \quad \theta_t = \theta_{t-1} + \omega_t.
$$
The class of second order polynomial models has $\mathbf{F} = (1, 0)^\top$ and
$\mathbf{G} = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ and the 
state vector $\boldsymbol{\theta}_t = (\mu_t, \beta_t)$ has two elements,
the first representing the level and the second the growth.

The general $p$-order polynomial models have a p-dimensional state vector,
$\mathbf{F} = (1, 0, \ldots, 0)^\top$, and $\mathbf{G}$ matrix $p\times p$
of Jordan form, where the diagonal and superdiagonal elements are all equal
to one, that is
$$
\mathbf{G} = \begin{pmatrix}
1 & 1 & 0 & \cdots & 0 \\
0 & 1 & 1 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{pmatrix}
$$
Then, the element $\theta_{t,r}$ in the state vector represents the $r$-th
derivative or difference in the series trend at time $t$. It undergoes
random fluctuations over time, influenced by the corresponding elements
of the innovations vector $\boldsymbol{\omega}_t$.

To define a $n$-th polynomial order in RBATS, we use the following
```
mod_polynomial <- dlm(polynomial = list(order = n, discount_factor = 0.95))
```


### Regressions 

$\mathbf{F}_t=(X_{t,1},\ldots,X_{t,p})^\top$ and $\mathbf{G}=\mathbf{I}_p$,
where $X_{t,i}$ denotes the $i$ covariate at time $t$ and $\mathbf{I}_p$
indicates the identity matrix of order $p$.


To define a dynamic regression model in `RBATS` we just need to use the following 
```
mod_regression <- dlm(regressor = list(xreg = X, discount_factor = 0.99))
```

#### Autoregressions

The basic representation of autoregression models of order $p$ in state space
form consider $\mathbf{F}_t = (y_{t-1}, \ldots, y_{t-p})^\top$ and
$\mathbf{G}=\mathbf{I}_p$.

Although not available in the current version of the `RBATS` package another
common representation has $\mathbf{F}_t = (1, 0, \ldots, 0)^\top$ and
$$
\mathbf{G} = \begin{pmatrix}
\phi_1 & 1 & 0 & \ldots & 0 \\
\phi_2 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\phi_{p-1} & 0 & 0 & \ldots & 1 \\
\phi_{p} & 0 & 0 & \ldots & 0 \\
\end{pmatrix}
$$
with $V_t = 0$ for all $t$ and $\mathbf{W}_t$ having zero entries except the
$\mathbf{W}_{1,1} = w_t > 0$.


### Seasonal

Modelling seasonal patterns in time series requires a component form that is
periodic. The simplest approach is the free form or the factor model, where a 
different factor is defined for each point in a cycle. This approach is not
parsimonious, which leads to the fourier form, where uses sines and cosines
to describe the cyclical behaviour of the data.

The current version of `RBATS` only supports one component of seasonality,
that is, it is not possible to include multiple seasonality such as weekly and
annual.

#### Form free

The form free approach to define a seasonal pattern of period $p$ has the
following components

$$
\mathbf{F}=(1,0,\ldots,0)^\prime
\quad \textrm{and} \quad
\mathbf{G}=\left(\begin{matrix} \mathbf{0}  &  \mathbf{I}_{p-1}  \\ 1 &
\mathbf{0}^\prime \end{matrix} \right).
$$

To define a dynamic seasonal with period $p$ using the form free we can use
```
mod_seas_free <- dlm(seasonal = list(type = "free", period = p, discount_factor = 0.98))
```


#### Fourier

In the Fourier approach the seasonal component of period $p$ is define
considering the $1, \ldots, m$ integer harmonics ($m < p$), where each one has
the form
$\mathbf{F}_j=(1, 0)^\top$ and
$$\mathbf{G}_j = \left(\begin{matrix} \cos(\omega_j)  &  \sin(\omega_j)  \\ -\sin(\omega_j)&  \cos(\omega_j)\end{matrix} \right)$$
for $j=1,\ldots, m$ and Fourier frequencies $\omega_j = 2\pi \, j / p$.

Hence, the full seasonal Fourier block with period $p$ and $1, \ldots, m$
harmonics is define as 
$\mathbf{F}=(\mathbf{F}_1, \ldots, \mathbf{F}_j)'$ and
$\mathbf{G} = \textrm{blockdiag}\left\{\mathbf{G}_1, \ldots, \mathbf{G}_{j}\right\}$

To define a dynamic seasonal using Fourier form with period $p$ and the first $k$ harmonics we can use
```
mod_seas_free <- dlm(seasonal = list(type = "fourier", period = p, harmonics = 1:k,
                                     discount_factor = 0.98))
```



## Sequential Inference

> This section describes how the sequential inference implemented in the `forward_filter` method works.

One main feature of Bayesian DLMs is the sequential inference based on
the Bayes' theorem. If the observation and evolution equation are known
the results coincide with the Kalman filter. Because of that, the
the sequential operation to obtain the prior, predictive and posterior
distribution is known as forward filter.
<!-- In `RBATS` the method `forward_filter` -->
<!-- for objects of class `dlm` perform this operation sequentially on time, -->
<!-- including the estimation of the variances, $V_t$ and $\mathbf{W}_t$. -->

The results presented below, which assume that $V_t$ and $\mathbf{W}_t$ are
known, are established in Chapter 4 of West and Harrison (1997).

*Evolution -- prior distribution*

\begin{equation*}
\left(\boldsymbol{\theta}_t \mid D_{t-1}\right) \sim N[\mathbf{a}_t, \mathbf{R}_t]
\end{equation*}
with $\mathbf{a}_t = \mathbf{G}\,\mathbf{m}_{t-1}$ and 
$\mathbf{R}_t=\mathbf{G}'\mathbf{C}_{t-1}\mathbf{G}_t + \mathbf{W}_t$.

*Forecast -- predictive distribution*

<!-- The one-step-ahead forecasting distribution can be obtained by integrating out -->
<!-- $\boldsymbol{\theta}_t$ from the joint distribution of -->
<!-- $y_t, \boldsymbol{\theta}_t$ given the -->
<!-- past observations $D_{t-1}$. Consequently it follows that: -->
\begin{equation*}
\left(Y_{t} \mid D_{t-1}\right) \sim N[f_t, Q_t]
\end{equation*}
where $f_t = \mathbf{F}'_t\,\mathbf{a}_t$ and
$Q_t=\mathbf{F}'_t\,\mathbf{R}_t\,\mathbf{F}_t + V_t$.


*Updating -- posterior distribution*

<!-- After observing $y_t$, using the prior distribution above and the normal -->
<!-- likelihood, given $V_t$, we have: -->
\begin{equation*}
\left(\boldsymbol{\theta}_t \mid D_t\right) \sim N[\mathbf{m}_t, \mathbf{C}_t]
\end{equation*}
with $\mathbf{m}_t = \mathbf{a}_t + \mathrm{A}_t\,e_t$ and
$\mathbf{C}_t = \mathbf{R}_t - \mathrm{A}_t\,\mathrm{A}'_t\,Q_t$, 
where $\mathrm{A}_t=\mathbf{R}_t\,\mathbf{F}_t / Q_t$ and $e_t= y_t-f_t$.


### Estimation of unknown time-varying observation variance

> This section explains how the package estimate the observational variance and what does the parameter `df_variance` when define a `dlm` controls.

In order to estimate the observation variance the normal-gamma distribution
is used to obtain a conjugate analysis. Also, to introduce a dynamic in the
observation variance we assume the following model for the $\phi_t = 1/V_t$,
the observation precision
\begin{eqnarray}
\phi_t &=& \gamma_t\,\phi_{t-1} / \beta, \quad \textrm{with} \quad \gamma_t \sim \mathrm{Beta}[\beta n_{t-1} / 2, (1- \beta)\,n_{t-1}/2] \\
(\phi_{0} \mid D_{0}) &\sim& \textrm{Gamma}(n_{0} / 2, d_{0} / 2)
\end{eqnarray}
where the preceding degrees of freedom ($n_0$) and the sum of squares ($d_0$),
respectively are prior parameters that should be specified.

The variance discount factor (`df_variance`), $\beta \in (0, 1]$, enables the variance to
evolve over time, and if it is equal to one, it leads to the constant variance
model, $V_t = V$, for all $t$. Typically, $\beta$ values are between 0.95 and
0.99 (see Chapter 10 of West and Harrison (1997)).

Unconditional to $V_t$, all of the previously presented distributions transform
into t-Student distributions (see Chapter 10 of West and Harrison (1997)).

### Specification of evolution variance matrix

> This section explain how the discount factor approach works, which is related to the parameter `discount_factor` in each model block.

To maintain the sequential inference feature, $\mathbf{W}_t$ is specified using
the **discount factor** technique. A single discount factor denoted by
$\delta \in (0,1]$ is a multiplicative factor useful to
**increase the uncertainty** in the evolution step of the sequential inference in DLMs.

Note that the prior variance is defined as $\mathbf{R}_t = \mathbf{G}_t^{\prime}\mathbf{C}_{t-1}\mathbf{G}_t + \mathbf{W}_{t} = \mathbf{P}_{t} + \mathbf{W}_{t}$
, where $\mathbf{P}_{t} = \mathbf{G}_t^{\prime}\mathbf{C}_{t-1}\mathbf{G}_t$.
For the static model $\mathbf{W}_t = 0$, so that $\mathbf{R}_t = \mathbf{G}_t^{\prime}\mathbf{C}_{t-1}\mathbf{G}_t$.
On the other hand, if the model is dynamic the precision $\mathbf{R}_t^{-1}$ is
reduced relative to the posterior at time $t-1$, i.e., $\mathbf{P}_{t}^{-1}$.
Then the intuitive way to express this relation is by

$$\mathbf{R}^{-1}_t = \delta \mathbf{P}_{t}^{-1} \rightarrow \mathbf{R}_t = \delta^{-1} \mathbf{P}_{t}$$
where $\delta \in (0, 1]$ is a discount factor. This implies that the system
variance can be specified as

$$\mathbf{W}_t = \dfrac{1-\delta}{\delta}\,\mathbf{P}_{t} = \dfrac{1-\delta}{\delta}\,\mathbf{G}_t^{\prime}\mathbf{C}_{t-1}\mathbf{G}_t.$$

In practice, a single discount factor is usually not appropriate, so different
discounts are used for each model component. That is, the evolution matrices,
$\mathbf{W}_{it}$, are determined by $\mathbf{W}_{it}= (1/\delta_i - 1) \, \mathbf{P}_{it}$, where
$\delta_i$ is the discount factor for the $i$-th component, for $i=1, \ldots, r$.
In addition, $\delta_i$ lies in $(0, 1]$ and its typical range for practical
analysis is $[0.9, 0.99]$. Clearly, if $\delta_i = 1$, then $\mathbf{W}_{it} = 0$ which
leads to the static model for the component $i$. Conversely, if
$\delta_i \approx 0$ the elements of $\mathbf{W}_{it}$ will be very large, resulting in
an unstable model component.

The automatic monitoring method also available in RBATS throughout the `dlm`
object is closely related to the specification of discount factor values.
In particular, model adaptation is carried out by raising the uncertainty of the
state parameters when a model breakdown is observed. We will discuss this below.


### Smoothing distribution

> This section describes what it is implemented in the `backward_smoother` method.

Let be $T$ the most recent observed time, the operation backward smoothing looks
back to time and revise the filtering distribution by using all available
information until time $T$, i.e., $D_{T}$.

The smoothed distribution for the state parameters are defined by
\begin{eqnarray}
(\boldsymbol{\theta}_t \mid D_T) \sim N [\mathbf{a}_T(t), \mathbf{R}_T(t)]
\end{eqnarray}
where
$\mathbf{a}_T(t) = \mathbf{m}_t + \mathbf{B}_t [\mathbf{a}_T(t+1) - \mathbf{a}_{t+1}]$,
$\mathbf{R}_T(t) = \mathbf{C}_t + \mathbf{B}_t [ \mathbf{R}_T(t) - \mathbf{R}_{t+1}] \mathbf{B}'_t$ and
$\mathbf{B}_t = \mathbf{C}_t \mathbf{G}'_{t+1} \mathbf{R}^{-1}_{t+1}$,
with starting values $\mathbf{a}_T(0) = \mathbf{m}_T$ and $\mathbf{R}_T(0) = \mathbf{C}_T$ (see West and Harrison (1997), Sec. 4.7). 

The corresponding smoothed distribution for the mean response of the series is
given by
\begin{eqnarray*}
    (\mu_{t} \mid D_T) \sim N[f_T(t), Q_T(t)]    
\end{eqnarray*}
where $f_T(t) = \mathbf{F}'_t \mathbf{a}_T(t)$ and
$Q_T(t) = \mathbf{F}'_t \mathbf{R}_T(t) \mathbf{F}_t$.

## Forecast distribution

> This section describes what is behind the `forecast` method for `dlm` objects.

For all integers $k \geq 0$, the following $k$-step marginal
distributions are defined:

- State distribution:
$$
(\boldsymbol{\theta}_{t+k} \mid D_t) \sim N[\mathbf{a}_t(k), \mathbf{R}_t(k)],
$$
- Forecast distribution:
$$
(Y_{t+k} \mid D_t) \sim N[f_t(k), q_t(k)],
$$
where
$$
f_t(k) = \mathbf{F}^{\top}_{t+k}\,\mathbf{a}_t(k) \quad \textrm{and} \quad
q_t(k) = \mathbf{F}^{\top}_{t+k}\,\mathbf{R}_{t}(k)\,\mathbf{F}_{t+k} + V_{t+k}
$$
with
\begin{eqnarray}
\mathbf{a}_t(k) &=& \mathbf{G}_{t+k}\,\mathbf{a}_t(k-1), \\
\mathbf{R}_t(k) &=& \mathbf{G}_{t+k}\,\mathbf{R}_t(k-1)\,\mathbf{G}^{\top}_{t+k} + \mathbf{W}_{t+k}
\end{eqnarray}
where $\mathbf{a}_t(0) = \mathbf{m}_t$ and $\mathbf{R}_t(0) = \mathbf{C}_t$.

We should emphasize some technical details:

1. $\mathbf{W}_{t+k}$ is usually unknown, then we replace it by the most recent discounted version, that is, $\mathbf{W}_{t}$.

2. The results presented assume that the observation variance is known, but in that is not common, then the distributions are replaced by t-Student with the degrees of freedom equal to $n_t$ and $V_t$ is replaced by the estimate $S_t$.

3. For the available models in `RBATS`, $G_t$ is constant at time implying that
\begin{eqnarray}
\mathbf{a}_t(k) &=& \mathbf{G}^k\,\mathbf{m}_t, \\
\mathbf{R}_t(k) &=& \mathbf{G}^k\,\mathbf{C}_t\,\mathbf{G}^k + \mathbf{W}_{t+k}.
\end{eqnarray}


## Automatic Sequential Monitoring

> This section describes a unique feature of the package which is the automatic sequential monitoring for detection of parametric changes or outliers.

The sequential analysis of DLMs offers great flexibility in continuously
evaluating the predictive performance of the current model and adapting it if 
potential model failures are detected. The general Bayesian automatic model 
monitoring and adaptation method was developed by \cite{West1986b} and 
extended to a broad class of dynamic models by \cite{West1986}. This technique
considers purely statistical measures of model performance, based on the 
concept of the local Bayes factor, and it involves the following main steps: 

1. Propose an alternative DLM $(M_1)$ describing a level and/or scale shift; 

2. Compute the Bayes factor, $H_t$ and update the cumulative Bayes factor $L_t$ and run-length $l_t$;

3. Check if the measures indicates potential model breakdown deviates;

4. If so, performs automatic interventions in order to model adapts itself.

### Bayes factor

At time $t$, the Bayes factor based on the most recent $k$ observations is given by

$$B_t(k) = H_t\,B_{t-1}(k-1) = \dfrac{p(y_t,\ldots, y_{t-k+1} \mid D_{t-k})}{p_A(y_t,\ldots, y_{t-k+1} \mid D_{t-k})}$$

where

$$H_t = \dfrac{p(y_t \mid D_{t-1})}{p_A(y_t \mid D_{t-1})}$$
and $p_A(y_t \mid D_t)$ is the predictive density distribution of the alternative model $M_A$

Given a suitable alternative model, small values of these Bayes factors
indicate poor predictive ability, or failure, of the standard model.

### Cumulative Bayes Factor

For $y_t$ alone, $H_t$ summarizes the possibility of major discontinuity,
that could be due an _outlier_ or structural/parametric change.

To assess the possibility of structural change, then two statistical measures based on the group of consecutive observations. The cumulative Bayes factor and the run-length, which  can be compute recursively by:
\begin{eqnarray*}
L_t&=& B_t \, \min_{1 \le k \le t}\{1, L_{t-1}\}\\ 
l_t&=& 1+ l_{t-1}\,\times \, I_{(-\infty,1)}(L_{t-1})
\end{eqnarray*}
where $B_t=B_t(0)$.


### Alternative model

One major concern in model monitoring is to define a suitable alternative model,
$\mathcal{M}_1$.

By using the linear transformation of Normal/t-Student family of distributions
we can propose different alternative models for the class of DLMs.
Without loss of generality, we shall monitor the standardised one-step ahead error,
that is, 
$$e_t = \dfrac{y_t - f_t}{\sqrt{q_t}},$$

which follows a standard Normal/t-Student distribution, depending on whether
the variance is known.

Thus, a natural alternative model for the DLM is
$$
\mathcal{M}_1: (e_t \mid D_{t-1}) \sim N(h, k^2)
$$
where $h$ and $k$ should be specifed by the user and describe the shifts in mean
and variance, respectively.

With this alternative model the Bayes factor is defined by
$$
B_t = k\exp\left[\dfrac{(e_t - h)^2 - k^2\,e_t^2}{2k^2} \right].
$$

### Automatic adaptation

There are two types of breakdown that the monitor can detect, based on the 
Bayes factor, cumulative Bayes factor and the run-length. For each one the
model adaptation will be slight different, although both are based on 
increase the uncertainty of the state parameters by decreasing the discount
factor. More precisely,

1. **Potential _outlier_**: This happen when $B_t < \tau$, where $\tau$ is 
a specified threshold. This is the most common and sometimes is confused with
the parametric change.

When detected a potential outlierthe observation is deemed as an
outlier and the observational variance is set to zero, which corresponds to
discarding the current observation, i.e., do not perform the Kalman update.

It should be clear that, without additional external information, it is 
impossible to sequentially distinguish a single _outlier_ from the onset of a 
structural change. Therefore, to prevent potential parametric changes in the 
future, we need to increase the uncertainty of the parameters by modifying the
default discount to smaller values.


2. **Parametric change**: The parametric change is detected when
$$
B_t < \tau \quad \textrm{and} \quad \left[L_t < \tau \ \ \textrm{or} \ \ l_t > r \right]
$$
When this type of breakdown happen the following is perform:

  i. Get back to time $t^\ast = t - l_t$.
  
  ii. Increase the uncertainty of the prior at $R_{t^\ast}$.
  
  iii. Perform the update until the time $t$.


### Operational parameters

Operational parameters that control the automatic monitoring and adaptation include the specification of the alternative model parameters, the level $h$ or scale $k$ shifts, the threshold $\tau$ for the Bayes factors, and the exceptional discount factors, $\delta_i, i = 1,\ldots,r$ for each model component.


the shift values $h$ and $k$, and a threshold value $\tau$, for which the Bayes factor indicates evidence against $\mathcal{M}_0$ in favour of $\mathcal{M}_1$.
To find an appropriate value for $\tau$, \cite{West1997} pointed out to test ranges of values of $h$ and $k$ that lead to interesting values of error $e_t$. 
For the alternative location model, i.e., $k=1$ and an error of $e_t = 1.65$ (the 95\% quantile of the $N(0,1)$ distribution) the case
when there is indifference between the models, i.e., $B_t = 1$ (or $\log B_t = 0$) gives the following equation, that solving for $h$ resulting in:

$$
\log B_t = 0.5 (h^2 - 2\,h\,e_t) \Rightarrow 0 = 0.5\,(h^2 - 2\,h\,e_t) \Rightarrow h = 2\,e_t
$$

For $e_t = 1.65$, we have that $h = 3.3$ leads to a Bayes factor of $1$. For $e_t = 2.33$, corresponding to the 99\% quantile we have that $B_t = 0.10$, hence a suitable threshold when $k=1$ and $h=3.3$ is $\tau=0.10$. The same idea can be applied for the scale and the location/scale models in order to have suitable values of $\tau$ for fixed shift values of $h$ and $k$.
Figure below shows the Bayes factor as a function of error for several alternative models. Obviously, as the error increases the Bayes factor decreases and there is no difference between the alternative models.


```{r bayes-factor, echo=FALSE}

bf <- function(e, h, k) {
  k * exp(((e - h)^2 - k^2 * e ^ 2) / 2 / k^2)
}


# Location alternative ----------------------------------------------------

e <- seq(1.6, 4, l = 100)
h <- c(2, 3, 3.3, 3.6, 4)
expanded_parms <- expand.grid(e = e, h = h)
expanded_parms$bf <- bf(e = expanded_parms$e, h = expanded_parms$h, k = 1)

chosen_loc <- expanded_parms[
  expanded_parms$bf <= 1.1 & expanded_parms$bf >= 1.0, ]

p_loc <- ggplot(expanded_parms, aes(x = e, y = bf, linetype = factor(h))) +
  geom_line(size = 1.0) + 
  geom_hline(yintercept = 1, col = "red") +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6)) +
  ggtitle("Alternative for location") +
  labs(x = bquote("|"~e[t]~"|"), y = "Bayes factor", linetype = "")


# Scale alternative -------------------------------------------------------

root_tau <- function(k, et) et^2 * (1/k^2 - 1) - 2 * log(k)
ks <- seq(2, 5, l = 10)
f <- root_tau(k = ks, et = 1.65)
# plot(ks, f)

e <- seq(1.6, 4, l = 100)
k <- c(2, 3, 4, 5)
expanded_parms <- expand.grid(e = e, k = k)
expanded_parms$bf <- bf(e = expanded_parms$e, h = 0, k = expanded_parms$k)

chosen_scale <- expanded_parms[
  expanded_parms$bf <= 1.1 & expanded_parms$bf >= 1.0, ]
p_scale <- ggplot(expanded_parms, aes(x = e, y = bf, linetype = factor(k))) +
  geom_line(size = 1.0) + 
  geom_hline(yintercept = 1, col = "red") +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6)) +
  ggtitle("Alternative for scale") +
  labs(x = bquote("|"~e[t]~"|"), y = "Bayes factor", linetype = "")


# Location and Scale alternatives -----------------------------------------
e <- seq(1.6, 4, l = 100)
k <- c(2, 3, 4, 5)
h <- c(3, 4)
expanded_parms <- expand.grid(e = e, h = h, k = k)
expanded_parms$bf <- bf(e = expanded_parms$e,
                        h = expanded_parms$h, k = expanded_parms$k)
expanded_parms$parms <- paste0("h = ", expanded_parms$h,
                               " k = ", expanded_parms$k)
chosen_loc_scale <- expanded_parms[
  expanded_parms$bf <= 1.1 & expanded_parms$bf >= 1.0, ]

p_loc_scale <- ggplot(expanded_parms,
                      aes(x = e, y = bf, col = factor(parms))) +
  geom_line(size = 1.0) + 
  geom_hline(yintercept = 1, col = "red") +
  colorspace::scale_color_discrete_diverging() +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6)) +
  ggtitle("Alternative for location and scale") +
  labs(x = bquote("|"~e[t]~"|"), y = "Bayes factor", col = "")


# Saving ------------------------------------------------------------------
p_grid <- plot_grid(p_loc, p_scale, p_loc_scale)

chosen_loc$k <- 1
chosen_scale$h <- 0

cols_ordered <- c("h", "k", "e", "bf")

tab <- dplyr::bind_rows(chosen_loc[, cols_ordered],
                        chosen_scale[, cols_ordered],
                        chosen_loc_scale[, cols_ordered])
# Printing
print(p_grid)
```
The Bayes factor values close to one are presented in Table below for different
values of $h$ and $k$. This could be help to know how much error the user will
accept when the models are indifference. For instance, with $h=4$ and $k=1$,
an error of $e_t = 1.794$	leads to a Bayes factor close to $1.022$, which means
that there is no difference between the models.
```{r table, echo=FALSE}
kableExtra::kbl(tab, digits = c(rep(0, 2), rep(3, 2))) |>
  kableExtra::kable_styling("hover", full_width = F)
```



Concerning the exception discount factors values, $\delta_i$, there is no
general rule for each value, depending on the application and the practioner
experience. Roughly speaking, we usually modify the discount factor of the
level component, since it has a quickly response on the model adpation than
other components, such as the seasonality. 


# Examples

## Flow of the River Nile

To analyse this data set we will consider the local level model and compare
the results with the `StructTS` function included in the `stats` package.
The `StructTS` function is simple to use and performs the maximum likelihood
estimation to obtain the observation and evolution variance, which are 
assumed to be constant in time. However, this function does not provide
standard errors of the estimates, which are required to compute the 
prediction interval.

```{r StructTS-nile}
fit_Nile <- StructTS(Nile, "level")
fit_Nile
```


We can obtain the posterior level (filtered) component with the `fitted` function and the
smoothing level using the `tsSmooth`, both methods should be applied to the
`fit_Nile` object. We will keep those information in a `data.frame` to compare
with the results from `RBATS`.
```{r organize-nile-data}
data_Nile <- data.frame(
  t = seq_along(Nile),
  time = seq.Date(as.Date("1871-01-01"), as.Date("1970-01-01"), by = "year"),
  y = c(Nile),
  filtered__StructTS = c(fitted(fit_Nile)),
  smoothed__StructTS = c(tsSmooth(fit_Nile))
)
```

The local level model in `RBATS` is defined considering the polynomial argument
in `dlm` with `order = 1`. Also, I will consider a discount factor of `0.80`
for the level and equal one for the variance, meaning that the observation
variance is constant at time. The `fit` method for the `dlm` object performs
the filtering and smoothing using the sequential Bayesian inference describe
above. Under the Bayes approach we need to specify prior moments for the 
level component. In this case, a vague prior with mean $1000$ and variance
$1000$ is specified.

```{r fit-nile-rbats}
model <- dlm(polynomial = list(order = 1, discount_factor = 0.80),
             df_variance = 1)
fit_model <- fit(model = model, y = c(Nile), a = matrix(1000, ncol = 1),
                 R = diag(1000, 1))
fit_model
```

```{r ploting}
data_Nile$filtered__RBATS <- fit_model$filtered$m[1, ]
data_Nile$smoothed__RBATS <- fit_model$smoothed$ak[1, ]
data_Nile_pivotted <- tidyr::pivot_longer(data_Nile, cols = -c(t, time, y)) |> 
  tidyr::separate(col = "name", into = c("distribution", "package"))
ggplot(data_Nile_pivotted, aes(x = time, y = y)) +
  facet_wrap(~distribution, ncol = 1) +
  geom_point(size = 1.5) +
  geom_line(aes(y = value, col = package), linewidth = 1) +
  labs(x = "Year", y = "Flow of the Nile river", col = "")
```

In fact, the Bayesian approach provides not only a point estimate, but a entire
predictive or smoothed distribution. We can explore this by using the method
`extract` applied to the `dlm.fit` objects.

```{r extract-nile}
data_predictive <- extract(x = fit_model, prob_interval = c(0.05, 0.20),
                           distribution = "filter", component = "response")
dplyr::glimpse(data_predictive)
data_predictive$time <- seq.Date(as.Date("1871-01-01"), as.Date("1970-01-01"),
                                 by = "year")
ggplot(data_predictive, aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = ci_lower__95, ymax = ci_upper__95), 
              fill = "blue", alpha = 0.2) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80), 
              fill = "blue", alpha = 0.1) +
  geom_point(size = 1.5) +
  geom_line(aes(y = mean), col = "blue") +
  labs(x = "Year", y = "Flow of river")

```


## Telephone calls

The telephone call dataset concerns the monthly average number of phone calls
in Cincinnati, USA. This dataset has three changes of regime. The first was in
early 1968, with impact lasting three months; the second was in the middle of
1973, less significant; and the third was in early 1974, more lasting.



```{r StructTS-telephone, include=FALSE}
data("telephone_calls")
fit_telephone <- StructTS(telephone_calls$average_daily_calls, "trend")
fit_telephone
plot(telephone_calls$average_daily_calls)
lines(fitted(fit_telephone)[, 1], col = "blue")
```

Because of that we will consider the local linear growth model using the automatic 
monitoring method, which aims to detect those regime change and perform
adaptation. The local linear growth is defined using considering the polynomial
order two. Also, we define another object where we specify the operational 
parameters regarding the monitoring method. Note that, when the monitor detects
any change the discount factor of the level will decrease from $0.90$ to $0.20$,
leading to more uncertainty on the level component and consequently a quick
adaptation in case of changes.

```{r fit-telephone}
model <- dlm(polynomial = list(order = 2, discount_factor = c(0.90, 0.95)),
             df_variance = 1)
fit_model <- fit(model = model, y = c(telephone_calls$average_daily_calls),
                 a = matrix(c(300, 0), ncol = 1),
                 R = diag(1000, 2))
model_monitor <- dlm(polynomial = list(order = 2, discount_factor = 0.90),
                     df_variance = 1,
                     monitor = list(
                       execute = TRUE, verbose = TRUE, start_time = 40L,
                       bilateral = TRUE,
                       bf_threshold = 0.135,
                       location_shift = 4, scale_shift = 1,
                       discount_factors = list(polynomial = c(0.2, 0.95))))
fit_model_monitor <- fit(model = model_monitor,
                         y = c(telephone_calls$average_daily_calls),
                         a = matrix(c(300, 0), ncol = 1),
                         R = diag(1000, 2))
```


After the fitting, for comparison purposes we extract the predictive
distribution and plot both with and without the monitor in the plot below.
```{r extract-telephone-predictive}
data_predictive <- extract(x = fit_model, prob_interval = c(0.05, 0.20),
                           distribution = "filter", component = "response")
data_predictive$time <- as.Date(telephone_calls$time)
data_predictive$monitor <- FALSE
data_predictive_monitor <- extract(x = fit_model_monitor,
                                   prob_interval = c(0.05, 0.20),
                                   distribution = "filter",
                                   component = "response")
data_predictive_monitor$time <- as.Date(telephone_calls$time)
data_predictive_monitor$monitor <- TRUE
data_predictive <- rbind(data_predictive, data_predictive_monitor)

ggplot(data_predictive[data_predictive$t > 5, ], aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = ci_lower__95, ymax = ci_upper__95, fill = monitor), 
              alpha = 0.2) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80, fill = monitor), 
              alpha = 0.1) +
  geom_point(size = 1.2) +
  geom_line(aes(y = mean, col = monitor)) +
  labs(x = "Year/Month", y = "Average daily calls")
```

In the next plot we compare the smooth level.
```{r extract-telephone-smooth}
data_state <- extract(x = fit_model, prob_interval = c(0.05, 0.20),
                      distribution = "smooth", component = "state")
data_state$time <- rep(as.Date(telephone_calls$time), times = 2)
data_state$monitor <- FALSE

data_state_monitor <- extract(x = fit_model_monitor,
                              prob_interval = c(0.05, 0.20),
                              distribution = "smooth", component = "state")
data_state_monitor$time <- rep(as.Date(telephone_calls$time), times = 2)
data_state_monitor$monitor <- TRUE

data_state <- rbind(data_state, data_state_monitor)

ggplot(data_state[data_state$parameter == "level", ],
       aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = ci_lower__95, ymax = ci_upper__95, fill = monitor),
              alpha = 0.2) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80, fill = monitor), 
              alpha = 0.1) +
  geom_point(size = 1.2) +
  geom_line(aes(y = mean, col = monitor)) +
  labs(x = "Year/Month", y = "Average daily calls")
```

## Monthly Airline Passenger Numbers

This example illustrates the use of a Fourier seasonal component. 

```{r fit-air-passengers}
model <- dlm(polynomial = list(order = 2, discount_factor = 0.95),
             seasonal = list(type = "fourier", period = 12,
                             harmonics = 1:2, discount_factor = 0.98))
fit_model <- fit(model = model, y = c(AirPassengers),
                 a = matrix(c(110, rep(0, 5)), ncol = 1),
                 R = diag(1000, 6))
```

```{r extract-air-passengers}
data_predictive <- extract(x = fit_model, prob_interval = c(0.05, 0.20),
                           distribution = "filter", component = "response")
data_predictive$time <- seq.Date(as.Date("1949-01-01"), as.Date("1960-12-01"),
                                 by = "month")

ggplot(data_predictive[data_predictive$t > 5, ], aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = ci_lower__95, ymax = ci_upper__95), fill = "blue",
              alpha = 0.2) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80), fill = "blue", 
              alpha = 0.1) +
  geom_point(size = 1.2) +
  geom_line(aes(y = mean), col = "blue") +
  labs(x = "Year/Month", y = "Airline passenger numbers")
```

```{r extract-state-air-passengers}
data_state <- extract(x = fit_model, prob_interval = c(0.05, 0.20),
                      distribution = "smooth", component = "state")
data_state$time <- rep(
  seq.Date(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "month"),
  times = nrow(model$FF) + 1)

chosen_states <- c("level", "growth", "sum_seasonality")
chosen_rows <- (data_state$parameter %in% chosen_states) & (data_state$t > 5)
ggplot(data_state[chosen_rows, ], aes(x = time, y = mean)) +
  facet_wrap(~parameter, ncol = 1, scales = "free") +
  geom_ribbon(aes(ymin = ci_lower__95, ymax = ci_upper__95), fill = "blue",
              alpha = 0.2) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80), fill = "blue", 
              alpha = 0.1) +
  geom_line(col = "blue") +
  labs(x = "Year/Month", y = "State")
```

We can derive the seasonal effects from the Fourier coefficients. In fact,
let $\boldsymbol{\theta}_{t,S}$ be the Fourier seasonal coefficients at time
$t$, then the seasonal effect of period $p$ at time $t$ denoted by 
$\boldsymbol{\phi}_t$ is the obtained by the following transformation
$$
\boldsymbol{\phi}_t = \mathbf{L}\boldsymbol{\theta}_{t,S} = \begin{pmatrix}
\mathbf{F}^\top \\
\mathbf{F}^\top\mathbf{G} \\
\vdots \\
\mathbf{F}^\top\mathbf{G}^{p-1} \\
\end{pmatrix}\,\boldsymbol{\theta}_{t,S}
$$
In fact, when define a DLM with Fourier seasonality the matrix $\mathbf{L}$ is
construct and kept in the model object. For this example we can access it by
```{r L-matrix}
model$L
```

Hence, the mean smoothed seasonal effects can be computed as follows:
```{r month-effects}
m_t <- fit_model$smoothed$ak
C_t <- fit_model$smoothed$Rk
L <- model$L
i_seas <- model$i_seasonal
mean_seas <- apply(m_t[i_seas, ], 2, function(x) L %*% x)
dim(mean_seas)
```

Therefore, there are 12 coefficients, representing each months, Jan, Feb, ..., 
and Dec. However, using the Fourier form we have estimated only $6$ parameters.

```{r plotting-seasonal-month-effect}
df_months <- as.data.frame(t(mean_seas))
colnames(df_months) <- month.abb
df_months$time <- seq.Date(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "month")
df_months_pivot <- df_months |> 
  tidyr::pivot_longer(cols = -time)

ggplot(df_months_pivot, aes(x = time, y = value, col = name)) +
  facet_wrap(~name) +
  geom_line(show.legend = FALSE)
```




## Daily electricity demand of Victoria, Australia

This example illustrates the use of regressors in DLMs. We will model the 
daily electricity demand as a function of the maximum temperature and 
dummy variables defining the type of the day, if it is holiday, weekday, or
weekend.

```{r electricity-data}
data("vic_electricity_daily")
X <- model.matrix(~ max_temperature + day_type, data = vic_electricity_daily)
X <- X[, -1, drop = FALSE]
X[, 1] <- (X[, 1] - mean(X[, 1])) / sd(X[, 1])
head(X)
```


This example shows how to the define the most complete DLM available in `RBATS`.
The model has three components.

1. The level growth model: 
```
polynomial = list(order = 2, discount_factor = 0.95)
```

2. The weekly seasonality represented by Fourier form with three first harmonics:
```
seasonal = list(type = "fourier", period = 7, harmonics = 1:3, discount_factor = 0.98)
```

3. The regressor component, with 3 covariates:
```
regressor = list(xreg = X, discount_factor = 0.99)
```

The state parameter $\boldsymbol{\theta}$ has dimension $11$. The first $2$
components are level and growth, the following $6$ are the harmonics from the
weekly seasonality and the last $3$ denotes the regression coefficients for
the maximum temperature, dummy for weekday and dummy for weekend, respectively.

```{r dlm-electricity}
model <- dlm(polynomial = list(order = 2, discount_factor = 0.95),
             seasonal = list(type = "fourier", period = 7,
                             harmonics = 1:3, discount_factor = 0.99),
             regressor = list(xreg = X, discount_factor = 0.99))
fit_model <- fit(model = model, y = vic_electricity_daily$demand,
                 a = matrix(c(175, rep(0, 10)), ncol = 1),
                 R = diag(1000, 11), smooth = TRUE)
```

The predictive distribution is shown below.
```{r predictive-elec}
data_predictive <- extract(x = fit_model, prob_interval = 0.20,
                           distribution = "filter", component = "response")
data_predictive$time <- vic_electricity_daily$time

ggplot(data_predictive[data_predictive$t > 2*11, ], aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80), fill = "blue", 
              alpha = 0.1) +
  geom_point(size = 1.2) +
  geom_line(aes(y = mean), col = "blue") +
  scale_x_date(breaks = scales::pretty_breaks(8)) +
  labs(x = "Day", y = "Daily electricity demand")
```

We can also inspect the state parameters of the model.
```{r extract-state-effect}
data_state <- extract(x = fit_model, prob_interval = 0.20,
                      distribution = "smooth", component = "state")
data_state$time <- rep(vic_electricity_daily$time,
                       times = nrow(model$GG) + 1)

chosen_states <- c("level", "growth", "max_temperature", "sum_seasonality")
chosen_rows <- (data_state$parameter %in% chosen_states) & (data_state$t > 22)
ggplot(data_state[chosen_rows, ], aes(x = time, y = mean)) +
  facet_wrap(~parameter, ncol = 1, scales = "free") +
  geom_ribbon(aes(ymin = ci_lower__80, ymax = ci_upper__80), fill = "blue", 
              alpha = 0.1) +
  geom_line(col = "blue") +
  labs(x = "Day", y = "State")
```

